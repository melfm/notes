# Backpropagation Numerical example

Assume the following network :

![Simple network with values](images/backprop_simpl_net.png)

The goal is to optimize the weights so that the net can learn how to correctly map arbitrary input to outputs.

![Initial values](images/simpl_net_init_vals.png)

Let's assume the above initial weights with the inputs as 0.05 and 0.1 and assume the desired output is 0.01 and 0.99.

## Forward pass
We start by feeding the values forward through the network.
A variable $z$ is defined as the total input to the hidden unit before the logistic nonlinearity.

\begin{align}
    z1 = (w1 \times x1 + w3 \times x2 + b1) \\
    z1 = (0.15 \times 0.05 + 0.25 \times 0.1 + 0.30) = 0.3325
\end{align}

We then apply the logistic activation function to get the output of $h_1$ :

\begin{equation}
  a_{h1} = \sigma (z1) = \frac{1}{1 + e ^ {- 0.3275}} = 0.5824 
\end{equation}

We repeat the same for $h2$.
\begin{align}
    z2 = (w2 \times x1 + w4 \times x2 + b1) \\
    z2 = (0.20 \times 0.05 + 0.3 \times 0.1 + 0.30) = 0.34 \\
    a_{h2} = \sigma (z2) = 0.5842
\end{align}

Now repeat for the output layer neuron, using the output from the hidden layer neurons as inputs.

\begin{align}
  out_{y1} = (w5 \times a_{h1} + w6 \times a_{h1} + b2)  \\
  out_{y1} = (0.4 \times 0.5824 + 0.45 \times 0.5842) + 0.7 = 1.196 \\
  \hat{y_1} = \sigma (out_{y1}) = 0.7676
\end{align}

Same for $\hat{y_2} = 0.7830$.

## Total error
\begin{equation}
  E_{total} = \sum \frac{1}{2}(target - output)^2
\end{equation}

NB. The $\frac{1}{2}$ is just there to cancel the differentiation later on.

\begin{equation}
  E_{y1} = \frac{1}{2}(t_1 - \hat{y_1})^2 = \frac{1}{2}(0.01 - 0.7676)^2 = 0.2869
\end{equation}

\begin{equation}
  E_{y2} = \frac{1}{2}(0.99 - 0.7830)^2 = 0.0214
\end{equation}

\begin{equation}
  E_{total} = E_{y1} + E_{y2} = 0.3083
\end{equation}

## Backward pass
Backpropagation updates each of the weights so that they cause the output to be closer to the target output, thereby minimizing the error for each output neuron and the network as a whole.

### Output layer
Consider $w5$. We want to know how much a change in $w5$ affects the total error : $\frac{\partial E_{total}}{\partial w5}$

Here the notation might be a bit mixed but whenever you see $out$ think of what comes out of the node having applied the activation, and otherwise think of it as what goes inside the node as input i.e. output of the node from previous layer (in this case its only our input).

Using the chain rule :

\begin{equation} 
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{y1}} \frac{\partial out_{y1}}{\partial y_1} \frac{\partial y_1}{\partial w5} 
\end{equation}

We now calculate each piece in this equation.
First, how much the total error change w.r.t the output :

\begin{align}
    E_{total} = \frac{1}{2}(t_1 - \hat{y_1})^2 + \frac{1}{2}(t_2 - \hat{y_2})^2 \\
    \frac{\partial E_{total}}{\partial out_{y1}} = 2 * \frac{1}{2} (t_1 - out_{y1})^{2-1} + 0 \\
    = -(t_1 - \hat{y_1}) = -(0.01 - 0.7676 ) = 0.7576
\end{align}

Next, how much output $out_{y1}$ changes w.r.t its total net input?
We are going to take partial derivative of the logistic function :

\begin{equation}
  \hat{y1} = \frac{1}{1 + e ^{-\hat{y1}}} \\
  \frac{\partial out_{y1}}{\partial y1} = out_{y1}(1- out_{y1}) = 0.7676(1 - 0.7676) = 0.1784
\end{equation}

Finally, how much the total net input of $y_1$ change w.r.t. $w5$?

\begin{align}
  {y1} = w5 \times out_{h1} + w6 \times out_{h2} + b2 \\
  \frac{\partial {y1}}{\partial w5} = 1  \times out_{h1} \times w5^{1-1} + 0 + 0 = 0.5824
\end{align}

Now we can put it all together :

\begin{equation} 
\frac{\partial E_{total}}{\partial w5} = \frac{\partial E_{total}}{\partial out_{y1}} \frac{\partial out_{y1}}{\partial y_1} \frac{\partial y_1}{\partial w5} 
\end{equation}

\begin{equation} 
\frac{\partial E_{total}}{\partial w5} = 0.7576 \times 0.1784 \times 0.5824 = 0.0787
\end{equation}

To decrease the error we subtract this value from the current weight multiplied by the selected learning rate (0.5) :

\begin{equation} 
    w5 = w5 - \alpha \times \frac{\partial E_{total}}{\partial w5} = 0.4 - 0.5 \times 0.0787 = 0.3607
\end{equation}

We then repeat this process to get the new weights, $w6 = ? $, $w7 = ? $ and $w8 = ? $ :

### Hidden Layer
Next, weâ€™ll continue the backwards pass by calculating new values for $w1$, $w2$, $w3$ and $w4$. Meanwhile going to watch our syntax here, I don't really like using the syntax $net$ but here to avoid getting confused, again, it means the input to the neuron.

![Hidden Layer backporpagation](images/backprop_hiddenL.png)


\begin{align} 
\frac{\partial E_{total}}{\partial w1} = \frac{\partial E_{total}}{\partial out_{h1}} \frac{\partial out_{h1}}{\partial net_{h1}} \frac{\partial net_{h1}}{\partial w1}  \\
\frac{\partial E_{total}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial out_{h1}} + \frac{\partial E_{o2}}{\partial out_{h2}}
\end{align}


Similar process however need to account for the fact that the output of each hidden layer neuron is going to contribute to both outputs. So in this case $out_{h1}$ is going to affect both $out_{o1}$ and $out_{o2}$, drawn with the red lines.


Starting with $\frac{\partial E_{o1}}{\partial out_{h1}}$, we already calculated the values :

\begin{align}
	\frac{\partial E_{o1}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial net_{o1}} \times \frac{\partial net_{o1}}{\partial out_{h1}} \\
	\frac{\partial E_{o1}}{\partial net_{o1}} = \frac{\partial E_{o1}}{\partial net_{o1}} \times \frac{\partial out_{o1}}{\partial net_{h1}} \\ 
	net_{o1} = w5 \times out_{h1} + w6 \times out_{h2} + b_2 \\
	\frac{\partial net_{o1}}{\partial out_{h1}} = w5 = 0.4
\end{align}

\begin{equation}
	\frac{\partial E_{o1}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial net_{o1}} * \frac{\partial net_{o1}}{\partial out_{h1}} = 0.7576 \times 0.1784 = 0.1352
\end{equation} 

Plugging these together:

\begin{equation}
	\frac{\partial E_{o1}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial net_{o1}} \times \frac{\partial net_{o1}}{\partial out_{h1}} = 0.1352 * 0.4 = 0.0541
\end{equation}

Follow the same process for $\frac{\partial E_{o2}}{\partial out_{h2}} = ?$

Therefore :
\begin{equation} 
	\frac{\partial E_{total}}{\partial out_{h1}} = \frac{\partial E_{o1}}{\partial out_{h1}} + \frac{\partial E_{o2}}{\partial out_{h2}} = ?
\end{equation}

Next, we need to figure out $\frac{\partial out_{h1}}{\partial net_{h1}}$ and then $\frac{\partial net_{h1}}{\partial w}$ for each weight.

\begin{align}
	out_{h1} = \frac{1}{1 + e ^{-net_{h1}}} \\
	\frac{\partial out_{h1}}{\partial net_{h1}} = out_{h1}(1 - out_{h1}) = 0.5824(1 - 0.5824) = 0.2432
\end{align}


We then calculate the partial derivative of the total net input to $h_1$ w.r.t. $w1$.

\begin{align}
	net_{h1} = (w1 \times x1 + w3 \times x2 + b1) \\
	\frac{\partial net_{h1}}{\partial w1} = x1 = 0.05
\end{align}

Now we can put it ALL together:

\begin{align}
	\frac{\partial E_{total}}{\partial w1} = \frac{\partial E_{total}}{\partial out_{h1}} \frac{\partial out_{h1}}{\partial net_{h1}} \frac{\partial net_{h1}}{\partial w1} \\
	\frac{\partial E_{total}}{\partial w1} = ? \times 0.2432 \times 0.05
\end{align}

Now we can update the weight:
\begin{equation}
	w1' = w1 - \mu * \frac{\partial E_{total}}{\partial w1} = 0.15 - 0.5 \times ? = ? 
\end{equation}

[Ref:Step by Step Backpropagation]

[Ref:Step by Step Backpropagation]: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

